{"cells":[{"cell_type":"code","execution_count":5,"id":"fbf2758f","metadata":{},"outputs":[{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@65f807a4\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","extractYear = SparkUserDefinedFunction($Lambda$4932/0x0000000801bf3840@3b0a429b,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n","metadataDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.rdd.RDD\n","import scala.util.parsing.json.JSON\n","import org.apache.spark.sql.DataFrame\n","import org.apache.spark.sql.SparkSession\n","import scala.util.Random\n","\n","\n","val spark = SparkSession.builder()\n","      .appName(\"EnrichMovieMetadata\")\n","      .master(\"local[*]\")\n","      .getOrCreate()\n","\n"," val moviesDF = spark.read.option(\"header\", \"true\").csv(\"gs://gcs_bucket_rupal/movies.csv\")\n","\n","val extractYear = udf((title: String) => {\n","  val yearPattern = \"\\\\((\\\\d{4})\\\\)\".r\n","  yearPattern.findFirstMatchIn(title).map(_.group(1)).getOrElse {\n","    (1980 + Random.nextInt(2024 - 1980 + 1)).toString\n","  }\n","})\n","\n","val metadataDF = moviesDF\n","  .select(\"movieId\", \"title\")\n","  .withColumn(\"releaseYear\", extractYear(col(\"title\")))\n","\n","metadataDF.coalesce(1) // Ensures a single output file\n","  .write\n","  .mode(\"overwrite\")\n","  .json(\"gs://gcs_bucket_rupal/metadata.json\")"]},{"cell_type":"code","execution_count":7,"id":"de7875ef","metadata":{},"outputs":[{"data":{"text/plain":["metadataPath = gs://gcs_bucket_rupal/metadata.json\n","metadataRDD = gs://gcs_bucket_rupal/metadata.json MapPartitionsRDD[24] at textFile at <console>:39\n","parsedMetadataRDD = MapPartitionsRDD[25] at map at <console>:42\n","metadataDF = [movieId: int, releaseYear: int]\n","moviesRDD = MapPartitionsRDD[26] at map at <console>:54\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<console>:44: warning: non-variable type argument String in type pattern scala.collection.immutable.Map[String,Any] (the underlying of Map[String,Any]) is unchecked since it is eliminated by erasure\n","           case Some(json: Map[String, Any]) =>\n","                           ^\n","<console>:43: warning: match may not be exhaustive.\n","It would fail on the following inputs: None, Some((x: Any forSome x not in scala.collection.immutable.Map[?,?]))\n","         JSON.parseFull(line) match {\n","                       ^\n","warning: one deprecation (since 1.0.6); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MapPartitionsRDD[26] at map at <console>:54"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","val metadataPath = s\"gs://gcs_bucket_rupal/metadata.json\"\n","val metadataRDD: RDD[String] = spark.sparkContext.textFile(metadataPath)\n","\n","val parsedMetadataRDD: RDD[(Int, Int)] = metadataRDD.map { line =>\n","  JSON.parseFull(line) match {\n","    case Some(json: Map[String, Any]) =>\n","      val movieId = json.get(\"movieId\").map(_.toString.toInt)\n","      val releaseYear = json.get(\"releaseYear\").map(_.toString.toInt)\n","      (movieId.get, releaseYear.get)\n","  }\n","}\n","\n","val metadataDF = parsedMetadataRDD.toDF(\"movieId\", \"releaseYear\")\n","\n","val moviesRDD: RDD[(Int, (String, String))] = moviesDF.rdd.map(row => {\n","  val movieId = row.getAs[Int](\"movieId\")\n","  val title = row.getAs[String](\"title\")\n","  val genres = row.getAs[String](\"genres\")\n","  (movieId, (title, genres))\n","})"]},{"cell_type":"code","execution_count":11,"id":"1ae53479","metadata":{},"outputs":[{"data":{"text/plain":["lastException = null\n"]},"metadata":{},"output_type":"display_data"},{"ename":"org.apache.spark.SparkException","evalue":"Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 9) (cluster-a693-m.us-central1-f.c.thermal-slice-441104-a0.internal executor driver): java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Integer (java.lang.String and java.lang.Integer are in module java.base of loader 'bootstrap')\n\tat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:103)\n\tat $line23.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$moviesRDD$1(<console>:55)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:173)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:","output_type":"error","traceback":["org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 9) (cluster-a693-m.us-central1-f.c.thermal-slice-441104-a0.internal executor driver): java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Integer (java.lang.String and java.lang.Integer are in module java.base of loader 'bootstrap')","\tat scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:103)","\tat $anonfun$moviesRDD$1(<console>:55)","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:173)","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)","\tat org.apache.spark.scheduler.Task.run(Task.scala:136)","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","\tat java.base/java.lang.Thread.run(Thread.java:829)","Driver stacktrace:","  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2717)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2653)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)","  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)","  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)","  at scala.Option.foreach(Option.scala:407)","  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2913)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2855)","  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2844)","  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","Caused by: java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Integer (java.lang.String and java.lang.Integer are in module java.base of loader 'bootstrap')","  at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:103)","  at $anonfun$moviesRDD$1(<console>:55)","  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)","  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:173)","  at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)","  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)","  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)","  at org.apache.spark.scheduler.Task.run(Task.scala:136)","  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)","  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)","  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)","  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)","  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)","  at java.base/java.lang.Thread.run(Thread.java:829)"]}],"source":["val enrichedRDD: RDD[(Int, (String, String))] = moviesRDD.leftOuterJoin(parsedMetadataRDD).mapValues { \n","    case ((title, genres), releaseYear) =>\n","        var enrichedTitled = title\n","        if (!title.matches(\".*\\\\(\\\\d{4}\\\\)$\")) {\n","            enrichedTitled = s\"$title (${releaseYear.get})\"\n","        }\n","        (enrichedTitled, genres)\n","}\n","\n","val moviesDF: DataFrame = enrichedRDD.map {\n","  case (movieId, (title, genres)) =>\n","    (movieId, title, genres)\n","}.toDF(\"movieId\", \"title\", \"genres\")\n","\n","val missingYearsCount = moviesDF.filter(!col(\"title\").rlike(\"\\\\(\\\\d{4}\\\\)$\")).count()\n","if (missingYearsCount > 0) {\n","  println(s\"Warning: $missingYearsCount movies missing releaseYear.\")\n","} else {\n","  println(\"All movies have a releaseYear.\")\n","}\n","\n","val outputParquetPath = \"hdfs://gcs_bucket_rupal/user//enriched-movies.parquet\"\n","moviesDF.write.mode(\"overwrite\").parquet(outputParquetPath)\n","\n","spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"df0f2f21","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}